
# Dataset Preâ€‘processingÂ & Graphâ€‘Building Pipeline

## Context
This repository hosts the **entire dataâ€‘ingestion toolâ€‘chain** for myÂ FYP  
*â€œSemanticâ€¯(Typeâ€‘4) Codeâ€‘Clone Detection via GNNs and Code2Vec Embeddingsâ€*.

* **Model codeâ€¯&â€¯evaluation** are already frozen.  
* This pipeline lets **anyone** recreate the exact training graphs with **one command**.

> **TL;DR**Â `python run_preprocess.py` takes raw GPTCloneBench data and  
> outputs readyâ€‘toâ€‘train `graphsâ€‘{train|val|test}/` folders.

---

## Pipeline Overview

```text
raw GPTCloneBench
        â”‚
        â–¼ â‘  shuffle_split_dataset.py
train / valid / test cloneâ€‘pair folders
        â”‚
        â–¼ â‘¡ split_clone_pair_methods_and_log_csv.py
one <method>.java per file  +  clone_pairs.csv
        â”‚
        â–¼ â‘¢ Java MethodValidator (jar)
validated processedâ€‘cloneâ€‘pairs/{train|val|test}
        â”‚
        â–¼ â‘£ code2vec_mod/preprocess.sh   (runs **inside WSL**)
ASTâ€‘path .c2v files  +  idâ€‘mapping
        â”‚
        â–¼ â‘¤ code2vec_strip_id_from_c2v_and_save_to_txtFile_ID.py
*.txt lists of ASTâ€‘path IDs  per split
        â”‚
        â–¼ â‘¥ check_id_ranges.py
verifies contiguous 0..N IDs (no gaps / dups)
        â”‚
        â–¼ â‘¦ convert_c2v_AST_paths_to_GNN_custom_embeddings.py {train|val|test}
PyGâ€‘ready graphsâ€‘{split}/  (binary `.pt`)
```

**WSL requirement**  
StageÂ â‘£ relies on original *code2vec* bash scripts.  
Windows users must enable **WindowsÂ SubsystemÂ forÂ Linux (WSL)** â€“ see video tutorial  
*[Install Windows Subsystem for Linux (WSL)](https://www.youtube.com/watch?v=gTf32sX9ci0&t=85s&ab_channel=TechWithCosta)*.

** Code2Vec preâ€‘trained model (required for StageÂ â‘£  & used again in Stage â‘¦) **

The Code2Vec tokenizer expects the original **Javaâ€‘14 model**.

1. Click the link below to download the 1.4â€¯GB archive:  
   [Download Code2Vec Javaâ€‘14 model](https://s3.amazonaws.com/code2vec/model/java14m_model.tar.gz)

2. Unzip it and place the extracted folder **exactly** here:

```
code2vec_mod/models/java14_model/
â”œâ”€â”€ dict.pkl
â”œâ”€â”€ model.keras
â””â”€â”€ â€¦ (other model files)
```

The preprocessing scripts are hardâ€‘coded to that path, so no extra flags
are needed.

---

## Stageâ€‘byâ€‘Stage Details

| # | Script / File | Responsibility |
|---|---------------|----------------|
| **â‘ ** | `shuffle_split_dataset.py` | Stratified shuffle of *true* and *false* semantic clone folders â†’ `trainâ€¯/â€¯validâ€¯/â€¯test` (60â€¯/â€¯20â€¯/â€¯20). |
| **â‘¡** | `split_clone_pair_methods_and_log_csv.py` | Extracts the **two topâ€‘level methods** from each cloneâ€‘pair file.<br>Generates one `.java` per **unique** method (hashâ€‘dedup) and logs pairs & labels to `clone_pairs.csv`. |
| **â‘¢** | *MethodValidatorÂ JAR* | Parses every `.java` to ensure it compiles as a valid method.<br>**Manual fixes applied:** syntax errors were corrected for each split (**trainÂ 10**, **validÂ 6**, **testÂ 4**).<br>If syntax errors remain, `code2vec/preprocess.sh` will skip those files causing missing graphs against `clone_pairs.csv`. |
| **â‘£** | `code2vec_mod/preprocess.sh` & `bootstrap_env.sh` | Original *code2vec* tokenizer â€” emits ASTâ€‘path `.c2v` files. Automatically runs in WSL on Windows, Bash on \*nix. |
| **â‘¤** | `code2vec_strip_id_from_c2v_and_save_to_txtFile_ID.py` | Extracts numeric IDs from `.c2v` headers â†’ compact `.txt` lists per split. |
| **â‘¥** | `check_id_ranges.py` | Verifies each splitâ€™s ID list is **contiguous starting atÂ 0** (no gaps / duplicates). |
| **â‘¦** | `convert_c2v_AST_paths_to_GNN_custom_embeddings.py` | For each split:<br>1. Loads `.c2v` AST paths<br>2. Encodes via a Code2Vec model<br>3. Saves PyTorchâ€‘Geometric graphs â†’ `graphsâ€‘{train|val|test}/`. |

`run_preprocess.py` chains all stages, detects Windows vsÂ Linux/macOS, prompts
before reâ€‘splitting, loops the validator until you type `YES`, and aborts on first
nonâ€‘zero exit code.

### Console prompts: split & validator

When you run **`python run_preprocess.py`** you will see two interactive checkpoints.

#### 1&nbsp;&nbsp;Split / Shuffle prompt
```text
Do you want to run the dataset splitting stage?
This will override any manual fixes. (y/n):
```
* **`y`**Â â†’ reâ€‘creates fresh `train / valid / test` folders.  
  Use only the **first** time (or when you intentionally want a new split).
* **`n`**Â â†’ keeps existing data **untouched** â€” recommended after youâ€™ve already
  corrected syntax errors or made other manual edits.

#### 2&nbsp;&nbsp;Methodâ€‘Validator confirmation
After each split is scanned youâ€™ll get:
```text
INPUT REQUIRED: Please enter 'YES' after youâ€™ve fixed every file listed above
or if no errors were reported (ready to continue):
```
* Open every file listed by the validator, patch syntax issues, **save**.
* When **all** errors are gone, type **`YES`** (uppercase/lowercase).  
  Only then will the pipeline advance to the Code2Vec `preprocess.sh` stage.

> Skipping this step (or typing anything other than `YES`) lets Code2Vec silently
> skip the bad files, which later causes missing graphs and runtime errors during
> training or inference.

### Next steps: training & inference

Once **all preprocessing stages complete successfully** you can move straight to
model work:

| Script | Purpose | Notes |
|--------|---------|-------|
| `CUDA_training_script.py` | Trains the GNN on the freshly-generated `graphs-train/ val/ test/` folders. | Requires a CUDA-enabled GPU and the environment specified in `requirements.txt`. |
| `CUDA_inference_script.py` | Runs inference on test `graphs-*` folder using the **pre-trained weights** bundled in the repo. | Can be executed immediately â€” training is not mandatory if you only want to reproduce baseline results. |

> **Tip:** if you train a new model, point the inference script to your checkpoint via  
> `--weights path/to/model.pt`.

---

## QuickÂ Start
### Set-up (run these in **Command Prompt / PowerShell**)

```cmd
:: clone the repo
git clone https://github.com/silvesterdan/clone-detection-pipeline.git
cd clone-detection-pipeline        :: enter project root

:: create & activate a fresh venv  (Windows/Linux)
python -m venv .venv

python -m venv .venv && .\.venv\Scripts\activate      # Windows
# source .venv/bin/activate                              # macOS/Linux

### Environment & installation options

By default we assume a **CUDA-capable GPU**.

```bash
# GPU setup
pip install -r requirements.txt          # full CUDA build
```

No CUDA?â€ƒTraining will still run (just slower).  
Create your virtual-env and install the two staged CPU lists instead:

```bash
pip install -r requirements_cpu_stage1.txt
pip install -r requirements_cpu_stage2.txt
```

(Stage 1 = common science deps, Stage 2 = PyTorch-CPU + PyG.)

---

### Output directory layout

After a successful run you should see:

```
dataset/
â””â”€ processed-clone-pairs/
   â”œâ”€ train/
   â”‚  â”œâ”€ 0.java 1.java â€¦          # individual methods
   â”‚  â””â”€                            (no CSV here)
   â”œâ”€ val/
   â”‚  â”œâ”€ â€¦ .java
   â”‚  â””â”€
   â””â”€ test/
      â”œâ”€ â€¦ .java
      â””â”€

   train_clone_pairs.csv            # CSVs live *beside* the folders
   val_clone_pairs.csv
   test_clone_pairs.csv

graphs-train/    *.pt               # PyG graphs (repo root)
graphs-val/      *.pt
graphs-test/     *.pt

```

**First run on Windows:** if WSL isnâ€™t installed the script will prompt you â€”  
follow the video guide *[linkâ€‘toâ€‘beâ€‘added]*, then reâ€‘run.

---

## Troubleshooting

| Symptom | Likely Cause | Fix |
|---------|--------------|-----|
| `bash not found` on Windows | WSL missing. | Install WSL (see video). |



PRs welcomeÂ ðŸ™‚



