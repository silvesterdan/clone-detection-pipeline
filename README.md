Dataset Pre-processing Pipeline â€” Current Status
===============================================

Context
-------
This repository now contains the cleaned up code for my *FYP*  of my Semantic (Typeâ€‘4)
Codeâ€‘Cloneâ€‘Detection project.  The core model & evaluation code is already
final (submitted). Iâ€™m currently polishing the **dataâ€‘ingestion** layer so
new users can reproduce results with a single command.

Pipeline Overview
-----------------
    raw GPTCloneBench folders
                â”‚
                â–¼  â‘  split_dataset.py
    train / valid / test cloneâ€‘pair files
                â”‚
                â–¼  â‘¡ prepare_clone_dataset.py
    individual <method>.java files  +  clone_pairs.csv

Stage details
-------------
  Stage â‘  â€“ split_dataset.py
      â€¢ Reads raw true_semantic_clones/ & false_semantic_clones/
      â€¢ Stratified shuffle â†’ train / valid / test (60 / 20 / 20)

  Stage â‘¡ â€“ prepare_clone_dataset.py
      â€¢ Extracts 2 topâ€‘level methods from every cloneâ€‘pair file
      â€¢ Deâ€‘duplicates(using method hashing) identical bodies across clone types (per split)
      â€¢ Saves one .java per unique method + clone_pairs.csv with labels

Both stages are chained by **run_preprocess.py** (tiny wrapper that just
calls them in order).

Quick start
-----------
    # clone repo & cd into it
    python -m venv .venv && source .venv/bin/activate   # optional
    pip install -r requirements.txt

    # 1â€‘click preprocessing
    python run_preprocess.py

    # â†’ produces:
    #   processed-clone-pairs/
    #     â”œâ”€â”€ train/
    #     â”‚   â”œâ”€â”€ 0.java 1.java â€¦
    #     â”‚   â””â”€â”€ clone-pairs-csv/clone_pairs.csv
    #     â””â”€â”€ valid/ â€¦   test/ â€¦

No commandâ€‘line flags are required yet (paths are hardâ€‘coded), but they will
be exposed in a future commit.

Why fewer .java files than before?
----------------------------------
The new pipeline shares one hash-map per split, so it de-duplicates method files across true and false semantic folders. If the exact same helper method appears in both, it is stored once and referenced by both rows in clone_pairs.csv.

In the old scripts each folder had its own hash-map and ID counter; the map was reset when the next folder started, so identical methods were saved twice (once per folder) and given different IDs. The shared map is why the file count now drops compared to those earlier, folder-specific dumps.



### Roadmap / cleanup checklist  (âœ… done Â· ðŸŸ¡ in-progress Â· ðŸ”² pending)

| Task | Status |
|------|--------|
| Final model & report (FYP submission) | âœ… |
| Consolidate preprocessing into `run_preprocess.py` | âœ… |
| Replace noisy per-file logging with per-split summaries | âœ… |
| Expose `--root` / `--seed` flags via `argparse` | ðŸŸ¡ |
| Add unit tests for brace-based method extractor | ðŸŸ¡ |
| Publish training + evaluation scripts in 	  | ðŸŸ¡ |
| **Pre-process Java files to AST paths with Code2Vec script** | ðŸ”² |
| **Convert AST-path files to embeddings using Code2Vec pretrained model** | ðŸ”² |
| **Build fully-connected PyG graphs & save locally** | ðŸ”² |
| **Training script for PyG graphs (epoch logs + final metric plots)** | ðŸ”² |
| **Inference script: load trained model, run on test split, print precision/recall/F1** | ðŸ”² |
| Write full usage guide in wiki | ðŸ”² |


Last updated: 9-05-2025
